# -*- coding: utf-8 -*-
"""DivSigma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LqNA2ZQMdi5VwNWurD9lWpDS-IUiHNhs

# Division Sigma 2020 Ignition Hacks
Authors: Rohit Ganti, Baladithya Balamurugan
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import re
from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import time
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# !rm contestant_judgment.csv 
# !wget https://raw.githubusercontent.com/Codeseys/IgnitionHacks2020DivSigma/master/contestant_judgment.csv 
# !rm training_data.csv 
# !wget https://raw.githubusercontent.com/Codeseys/IgnitionHacks2020DivSigma/master/training_data.csv

df_train = pd.read_csv('training_data.csv')
df_train.head()

stopwordslist =  set(stopwords.words('english'))
lemmatty = WordNetLemmatizer()
wordnetmap = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV}
def removestopwords(text):    
    return " ".join([word for word in str(text).split() if word not in stopwordslist])
def lemmy(text):
    pos_text = nltk.pos_tag(str(text).split())
    return " ".join([lemmatty.lemmatize(word,wordnetmap.get(pos[0],wordnet.NOUN))for word, pos in pos_text])    

def clean_text(text):
    text = re.sub(r'@[A-Za-z0-9]+','',text)
    text = re.sub(r'#','',text)
    
    text = re.sub(r'RT[\s]+','',text)
    text = re.sub(r'https?:\/\/\S+','',text)
    
    text = lemmy(text)
    text = text.lower()
    text = re.sub(r'[^\w\s]','', text)
    text = removestopwords(text)
    
    return text

# df_train['Text']=df_train['Text'].apply(clean_text)
# df_train.head()

TextBlob(df_train['Text'][1]).words

# df_train.to_csv('newcsvtest.csv')
SEED=114
x = df_train['Text']
y = df_train['Sentiment']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=SEED)

print("xTrain:",x_train.shape,"yTrain:",y_train.shape)
print("xTest:",x_test.shape,"yTest:",y_test.shape)

def vectorizer(data):

  start = time.time()

  tfidf = TfidfVectorizer(analyzer=clean_text)
  tfidfvect = tfidf.fit_transform(data)

  end = time.time()
  hours, rem = divmod(end-start, 3600)
  minutes, seconds = divmod(rem, 60)
  print("--- {:0>2}:{:0>2}:{:05.2f} ---".format(int(hours),int(minutes),seconds))

  print(tfidfvect.shape)
  return pd.DataFrame(tfidfvect.toarray(),columns=tfidf.get_feature_names())

x_train_tfidf=vectorizer(x_train)
x_test_tfidf=vectorizer(x_test)

print(x_train_tfidf.head())
print(x_test_tfidf.head())
print(y_train.head())
print(y_test.head())

logreg = LogisticRegression(max_iter=5000,solver='sag',random_state=SEED,C=1).fit(x_train_tfidf.astype('float64'),y_train)

y_pred = logreg.predict(x_test_tfidf)
result = classification_report(y_test,y_pred)
print(result)
score= logisticReg.score(X_test, y_test)
print("Score of our model is: " +str(score))

df= df_train()

cvec= TfidfVectorizer( analyzer='word', stop_words= 'english', max_features=7500, lowercase= True)
documents= df['Text'].values.astype('U')
X= cvec.fit_transform(documents).toarray()
words= cvec.get_feature_names()
y=df['Sentiment']
X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, shuffle= True)

start= timer();

logreg= LogisticRegression(max_iter=500)
logreg.fit(X_train, y_train)

y_pred= logreg.predict(X_test)
result= classification_report(y_test, y_pred)
print(result)
end= timer();


score= logisticReg.score(X_test, y_test)
print("Score of our model is: " +str(score))
print("Time for the model is: " +str(end-start))

